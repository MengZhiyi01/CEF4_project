# Futures Price Prediction with FNN and transformer

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A comprehensive deep learning framework for futures price prediction using Transformer and Feed-Forward Neural Network architectures. This project implements state-of-the-art techniques for short-term price forecasting in Chinese futures markets using high-frequency trading data.

## Table of Contents

- [Features](#features)
- [Performance](#performance)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Models](#models)
- [Training](#training)
- [Evaluation](#evaluation)
- [Configuration](#configuration)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## Features

###  **Advanced Model Architectures**
- **Time-Aware Transformer**: Custom transformer architecture with trading session awareness
- **Feed-Forward Neural Network**: Robust baseline model with advanced regularization
- **Attention Mechanisms**: Multi-head attention with temporal embeddings

###  **Comprehensive Data Processing**
- **High-Frequency Data**: Processes 1-minute trading data from Chinese futures markets
- **Feature Engineering**: 131 engineered features including technical indicators and microstructure variables
- **Robust Preprocessing**: Handles missing values, outliers, and numerical stability

###  **Production-Ready Framework**
- **Modular Design**: Clean, maintainable code architecture
- **Configuration Management**: JSON-based configuration system
- **Experiment Tracking**: Comprehensive logging and result management
- **Reproducibility**: Deterministic training with seed control

###  **Advanced Evaluation**
- **Financial Metrics**: Information Coefficient (IC), Sharpe ratio, directional accuracy
- **Statistical Testing**: Significance testing and confidence intervals
- **Visualization**: Automated result visualization and analysis

## Performance

###  **Key Results**

| Model | IC | P-value | RMSE | Prediction Diversity |
|-------|----|---------|----- |-------------------|
| **Transformer** | **0.0314** | **< 0.0001** | 0.765 | 402,145 unique values |
| FNN Baseline | 0.0240 | 0.0003 | 0.632 | 156,234 unique values |

###  **Model Comparison**

- **30.8% improvement** in Information Coefficient over baseline
- **Statistically significant** predictions (p-value < 0.0001)
- **Solved constant prediction problem** that commonly affects financial deep learning
- **High prediction diversity** ensuring meaningful forecasts

## Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (optional but recommended)
- 8GB+ RAM
- 2GB+ storage space

### Setup

1. **Clone the repository**:
```bash
git clone https://github.com/your-username/futures-prediction.git
cd futures-prediction
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Verify installation**:
```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
python -c "import pandas as pd; print(f'Pandas version: {pd.__version__}')"
```

## Quick Start

###  **5-Minute Demo**

Run the interactive demo to see the model in action:

```bash
python demo_transformer.py
```

**Expected output:**
```
============================================================
 TRANSFORMER FUTURES PREDICTION DEMO
============================================================
 Using device: cuda
 Loading data from: final_filtered_data_1min.parquet
 Using demo dataset: 50,000 samples

 Starting Demo Training...
--- Epoch 1/3 ---
 Training Epoch 1...
 Epoch 1 Summary:
   Train Loss: 1.012180
   Val IC: 0.0067

 Final Test Evaluation:
 Test Results:
   IC: 0.0457 (p-value: 0.0005)
   RMSE: 0.619157
   Prediction Diversity: 3,154 unique values
   Model Status:  WORKING

 DEMO COMPLETED SUCCESSFULLY!
```

###  **Full Training**

Train the complete model:

```bash
# Train transformer model
python train_transformer.py --epochs 10 --batch_size 256

# Train baseline FNN
python train_fnn.py --epochs 50 --batch_size 512
```

###  **Custom Configuration**

```bash
# Use custom configuration
python train_transformer.py --config config/config_transformer.json

# With specific parameters
python train_transformer.py --lr 0.0001 --batch_size 128 --epochs 20
```

## Project Structure

```
futures_prediction/
â”œâ”€â”€ ðŸ“ src/                          # Core source code
â”‚   â”œâ”€â”€ ðŸ“ models/                   # Model implementations
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ transformer.py        # Transformer architecture
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ fnn.py                # Feed-forward network
â”‚   â”‚   â””â”€â”€ ðŸ“„ __init__.py
â”‚   â”œâ”€â”€ ðŸ“ utils/                    # Utility functions
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ metrics.py            # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ logger.py             # Logging utilities
â”‚   â”‚   â””â”€â”€ ðŸ“„ __init__.py
â”‚   â”œâ”€â”€ ðŸ“ data/                     # Data processing
â”‚   â””â”€â”€ ðŸ“„ trainer.py                # Training orchestrator
â”œâ”€â”€ ðŸ“ experiments/                  # Experiment results
â”‚   â”œâ”€â”€ ðŸ“ transformer_final_stable/ # Best model results
â”‚   â””â”€â”€ ðŸ“ [other experiments]/
â”œâ”€â”€ ðŸ“ config/                       # Configuration files
â”‚   â”œâ”€â”€ ðŸ“„ config_transformer.json   # Transformer config
â”‚   â”œâ”€â”€ ðŸ“„ config_demo.json          # Demo configuration
â”‚   â””â”€â”€ ðŸ“„ config.yaml               # General config
â”œâ”€â”€ ðŸ“ docs/                         # Documentation
â”‚   â”œâ”€â”€ ðŸ“„ ACADEMIC_PROJECT_OVERVIEW.md
â”‚   â”œâ”€â”€ ðŸ“„ PROJECT_GUIDE.md
â”‚   â””â”€â”€ ðŸ“„ [other docs]/
â”œâ”€â”€ ðŸ“„ train_transformer.py          # Main training script
â”œâ”€â”€ ðŸ“„ demo_transformer.py           # Demo script
â”œâ”€â”€ ðŸ“„ train_fnn.py                  # FNN training script
â”œâ”€â”€ ðŸ“„ requirements.txt              # Dependencies
â””â”€â”€ ðŸ“„ README.md                     # This file
```

## Models

###  **Transformer Architecture**

```python
class FuturesTransformer(nn.Module):
    """
    Time-aware Transformer for futures price prediction
    
    Features:
    - Multi-head self-attention (4 heads)
    - Trading session embeddings (day/night/non-trading)
    - Positional encoding with temporal awareness
    - 2-layer encoder with 64-dimensional embeddings
    """
    
    def __init__(self, input_dim=131, d_model=64, n_heads=4, n_layers=2):
        # Implementation details...
```

**Key Components:**
- **Input Projection**: 131 features â†’ 64 dimensions
- **Temporal Embeddings**: Hour + trading session awareness
- **Multi-Head Attention**: 4 attention heads for pattern recognition
- **Feed-Forward Network**: 128 hidden units with GELU activation
- **Output Layer**: Global average pooling + linear projection

###  **Feed-Forward Network**

```python
class FNN(nn.Module):
    """
    Robust baseline model with advanced regularization
    
    Features:
    - Multi-layer architecture [256, 128, 64]
    - Batch normalization for stability
    - Dropout regularization
    - Advanced weight initialization
    """
```

**Architecture:**
- **Input Layer**: 131 features
- **Hidden Layers**: [256, 128, 64] with ReLU activation
- **Regularization**: Dropout (0.2) + Batch Normalization
- **Output Layer**: Single regression output

## Training

###  **Training Strategy**

The project implements a robust training approach that solves the common "constant prediction" problem in financial deep learning:

```python
# Key training components
optimizer = AdamW(lr=0.00005, weight_decay=0.01)
scheduler = ReduceLROnPlateau(patience=2, factor=0.7)
gradient_clipping = 0.5  # Conservative clipping
prediction_monitoring = True  # Real-time diversity tracking
```

###  **Numerical Stability**

1. **Double Precision Preprocessing**: Uses float64 for preprocessing
2. **Conservative Gradient Clipping**: max_norm=0.5 to prevent exploding gradients
3. **Prediction Variance Monitoring**: Real-time tracking of prediction diversity
4. **Adaptive Learning Rate**: Reduces learning rate when validation loss plateaus

###  **Training Loop**

```python
def train_epoch(model, train_loader, optimizer, device):
    model.train()
    for batch_idx, (features, labels, timestamps, metadata) in enumerate(train_loader):
        # Forward pass
        outputs = model(features, timestamps)
        loss = nn.MSELoss()(outputs.squeeze(), labels.squeeze())
        
        # Backward pass with stability checks
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        
        # Skip if gradients explode
        if grad_norm > 10.0:
            continue
            
        optimizer.step()
```

## Evaluation

###  **Financial Metrics**

#### Information Coefficient (IC)
```python
def calculate_ic(y_true, y_pred):
    """
    Calculate Information Coefficient (Pearson correlation)
    
    Returns:
        ic: Correlation coefficient
        p_value: Statistical significance
    """
    ic, p_value = pearsonr(y_true, y_pred)
    return ic, p_value
```

#### Prediction Quality Assessment
```python
def assess_prediction_quality(predictions):
    """
    Comprehensive prediction quality metrics
    
    Returns:
        - Variance and standard deviation
        - Prediction range and unique count
        - Constant prediction detection
    """
    return {
        'variance': np.var(predictions),
        'std': np.std(predictions),
        'unique_count': len(np.unique(predictions)),
        'is_constant': np.std(predictions) < 1e-6
    }
```

###  **Success Criteria**

| Metric | Threshold | Description |
|--------|-----------|-------------|
| IC | > 0.02 | Minimum correlation for significance |
| P-value | < 0.05 | Statistical significance |
| Prediction Std | > 1e-6 | Avoid constant predictions |
| Unique Predictions | > 1000 | Ensure prediction diversity |

###  **Evaluation Pipeline**

```python
# Automated evaluation workflow
def evaluate_model(model, test_loader, label_scaler):
    # 1. Generate predictions
    predictions, labels = get_predictions(model, test_loader)
    
    # 2. Inverse transform if needed
    if label_scaler:
        predictions = label_scaler.inverse_transform(predictions)
        labels = label_scaler.inverse_transform(labels)
    
    # 3. Calculate metrics
    ic, p_value = calculate_ic(labels, predictions)
    rmse = calculate_rmse(labels, predictions)
    
    # 4. Assess prediction quality
    quality = assess_prediction_quality(predictions)
    
    return {
        'ic': ic,
        'p_value': p_value,
        'rmse': rmse,
        'prediction_quality': quality
    }
```

## Configuration

###  **Model Configuration**

```json
{
    "model": {
        "d_model": 64,
        "n_heads": 4,
        "n_layers": 2,
        "d_ff": 128,
        "dropout": 0.3,
        "max_seq_len": 20
    },
    "training": {
        "batch_size": 256,
        "learning_rate": 0.00005,
        "num_epochs": 10,
        "gradient_clip_value": 0.5,
        "weight_decay": 0.01
    },
    "data": {
        "seq_len": 20,
        "feature_start_idx": 2,
        "feature_end_idx": 133,
        "label_col": "label_vwap_5m"
    }
}
```

###  **Command Line Arguments**

```bash
# Training arguments
--epochs N            # Number of training epochs (default: 10)
--batch_size N        # Batch size (default: 256)
--lr FLOAT           # Learning rate (default: 0.00005)
--seq_len N          # Sequence length (default: 20)
--gpu N              # GPU device number (default: 0)
--seed N             # Random seed (default: 42)
--save_dir PATH      # Results directory (default: ./experiments)
--config PATH        # Configuration file path

# Data arguments
--data_subset N      # Use subset of data (0 for full dataset)
--test_split FLOAT   # Test split ratio (default: 0.2)
--val_split FLOAT    # Validation split ratio (default: 0.1)
```

## Results

###  **Model Performance**

#### Transformer Model (Final)
```
 Training completed successfully!
 Test Results:
   IC: 0.031392 (p-value: < 0.0001)
   RMSE: 0.765
   Prediction Std: 0.074340
   Unique Predictions: 402,145
   Model Status: âœ… WORKING
```

#### FNN Baseline
```
 Baseline Results:
   IC: 0.024 (p-value: 0.0003)
   RMSE: 0.632
   Prediction Std: 0.045
   Unique Predictions: 156,234
   Model Status: âœ… WORKING
```

###  **Performance Comparison**

| Metric | Transformer | FNN | Improvement |
|--------|-------------|-----|-------------|
| IC | 0.0314 | 0.0240 | +30.8% |
| P-value | < 0.0001 | 0.0003 | +99.7% |
| Pred Std | 0.074 | 0.045 | +64.4% |
| Unique Pred | 402K | 156K | +157.7% |

###  **Key Achievements**

1. **Solved Constant Prediction Problem**: Achieved high prediction diversity
2. **Statistical Significance**: p-value < 0.0001 for test results
3. **Robust Architecture**: Stable training across multiple runs
4. **Production Ready**: Comprehensive evaluation and monitoring

###  **Training Progression**

```
Epoch 1: Train Loss: 1.012, Val IC: 0.0067, Val Pred Std: 0.000009
Epoch 2: Train Loss: 1.001, Val IC: -0.0049, Val Pred Std: 0.000006
Epoch 3: Train Loss: 0.999, Val IC: -0.0028, Val Pred Std: 0.000057
...
Best Model: Epoch 5 with Val Pred Std: 0.114517
Final Test: IC: 0.031392, Status: âœ… SUCCESS
```

## Advanced Usage

###  **Custom Model Development**

```python
# Create custom transformer variant
class CustomTransformer(FuturesTransformer):
    def __init__(self, input_dim, **kwargs):
        super().__init__(input_dim, **kwargs)
        # Add custom components
        self.custom_attention = CustomAttentionLayer()
        
    def forward(self, x, timestamps):
        # Custom forward pass
        x = self.custom_attention(x)
        return super().forward(x, timestamps)
```

###  **Hyperparameter Tuning**

```python
# Automated hyperparameter search
import optuna

def objective(trial):
    config = {
        'd_model': trial.suggest_categorical('d_model', [32, 64, 128]),
        'n_heads': trial.suggest_categorical('n_heads', [2, 4, 8]),
        'learning_rate': trial.suggest_float('lr', 1e-5, 1e-3, log=True),
        'dropout': trial.suggest_float('dropout', 0.1, 0.5)
    }
    
    return train_and_evaluate(config)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

###  **Production Deployment**

```python
# Model serving for real-time inference
class ModelServer:
    def __init__(self, model_path, scaler_path):
        self.model = self.load_model(model_path)
        self.scaler = self.load_scaler(scaler_path)
        
    def predict(self, features):
        # Preprocess features
        features_scaled = self.scaler.transform(features)
        
        # Generate prediction
        with torch.no_grad():
            prediction = self.model(features_scaled)
            
        return prediction.numpy()
```